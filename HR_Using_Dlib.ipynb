{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c08d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\aakas\\\\CV_Project\\\\HR_using_dlib'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cce71cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Using cached scipy-1.10.1-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\aakas\\.conda\\envs\\em\\lib\\site-packages (from scipy) (1.22.3)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ae7490",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "204d4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "\n",
    "# Load face detector from dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Read in and simultaneously preprocess video\n",
    "def read_video(path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    video_frames = []\n",
    "    face_rects = ()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, img = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        roi_frame = img\n",
    "\n",
    "        # Detect face\n",
    "        if len(video_frames) == 0:\n",
    "            dets = detector(gray, 1)\n",
    "            face_rects = [(d.left(), d.top(), d.right()-d.left(), d.bottom()-d.top()) for d in dets]\n",
    "\n",
    "        # Select ROI\n",
    "        if len(face_rects) > 0:\n",
    "            for (x, y, w, h) in face_rects:\n",
    "                roi_frame = img[y:y + h, x:x + w]\n",
    "            if roi_frame.size != img.size:\n",
    "                roi_frame = cv2.resize(roi_frame, (500, 500))\n",
    "                frame = np.ndarray(shape=roi_frame.shape, dtype=\"float\")\n",
    "                frame[:] = roi_frame * (1. / 255)\n",
    "                video_frames.append(frame)\n",
    "\n",
    "    frame_ct = len(video_frames)\n",
    "    cap.release()\n",
    "\n",
    "    return video_frames, frame_ct, fps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da0a70a",
   "metadata": {},
   "source": [
    "* This code defines a function read_video that takes a video path as input and returns a tuple consisting of the video frames, frame count, and frames per second (fps). \n",
    "\n",
    "* The function reads in the video using OpenCV's cv2.VideoCapture method and preprocesses each frame by converting it to grayscale. \n",
    "\n",
    "* It then detects the face in the first frame of the video using dlib's face detector and selects the region of interest (ROI) containing the face. \n",
    "\n",
    "* The ROI is resized to 500x500, normalized to values between 0 and 1, and added to the list of video frames. \n",
    "\n",
    "* This process is repeated for each frame in the video until all frames have been processed.\n",
    "\n",
    "* It is important to note that this code assumes there is only one face in the video, and that it is present in the first frame. * If there are multiple faces, or if the face is not present in the first frame, the code may not work as expected. \n",
    "\n",
    "* Additionally, the face detector used in this code is not perfect, so it may not always accurately detect the face in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25019b3a",
   "metadata": {},
   "source": [
    "* Import the dlib module\n",
    "* Load the face detector using dlib.get_frontal_face_detector()\n",
    "* Replace the OpenCV face detector faceCascade.detectMultiScale() with the dlib face detector detector(). \n",
    "* The output of detector() is a list of dlib.rectangle objects, which need to be converted to (x, y, w, h) format that is used by OpenCV functions.\n",
    "* Convert the dlib.rectangle objects to (x, y, w, h) format and store them in face_rects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6ec25",
   "metadata": {},
   "source": [
    "Explaination of above code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbddd10c",
   "metadata": {},
   "source": [
    "* First, we import the necessary modules: OpenCV (cv2), NumPy (numpy), and dlib (dlib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f0192c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f3dec",
   "metadata": {},
   "source": [
    "* Next, we load the face detector from dlib using the dlib.get_frontal_face_detector() function. \n",
    "* This returns a face detector object that we can use to detect faces in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1770240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector = dlib.get_frontal_face_detector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a07cf8",
   "metadata": {},
   "source": [
    "* This function takes a video path as input and initializes some variables: a cv2.VideoCapture object cap to read the video frames, \n",
    "* an integer fps to store the frames per second of the video, an empty list video_frames to store the processed frames, and an empty tuple face_rects to store the face bounding box coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d347d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_video(path):\n",
    "#     cap = cv2.VideoCapture(path)\n",
    "#     fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "#     video_frames = []\n",
    "#     face_rects = ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e09bdd",
   "metadata": {},
   "source": [
    "* This loop iterates through each frame in the video until all frames have been processed. \n",
    "* For each frame, it reads the frame using cap.read(), converts it to grayscale using cv2.cvtColor(), \n",
    "* and sets roi_frame to be the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9353d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     while cap.isOpened():\n",
    "#         ret, img = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "#         roi_frame = img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506184ea",
   "metadata": {},
   "source": [
    "* This code only runs for the first frame of the video, since len(video_frames) is 0 initially. \n",
    "* It uses the detector object to detect faces in the grayscale image, and converts the face bounding box coordinates from dlib.rectangle objects to tuples of left, top, width, and height values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322e1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         if len(video_frames) == 0:\n",
    "#             dets = detector(gray, 1)\n",
    "#             face_rects = [(d.left(), d.top(), d.right()-d.left(), d.bottom()-d.top()) for d in dets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d158d52",
   "metadata": {},
   "source": [
    "* This code runs for each frame after the first frame. \n",
    "* It selects the ROI containing the face using the bounding box coordinates, and resizes it to 500x500 using cv2.resize(). \n",
    "* It then normalizes the pixel values to be between 0 and 1, and stores the result in a numpy array frame, which is added to the video_frames list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a0bdc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         if len(face_rects) > 0:\n",
    "#             for (x, y, w, h) in face_rects:\n",
    "#                 roi_frame = img[y:y + h, x:x + w]\n",
    "#             if roi_frame.size != img.size:\n",
    "#                 roi_frame = cv2.resize(roi_frame, (500, 500))\n",
    "#                 frame = np.ndarray(shape=roi_frame.shape, dtype=\"float\")\n",
    "#                 frame[:] = roi_frame * (1. / 255)\n",
    "#                 video_frames.append(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b86971",
   "metadata": {},
   "source": [
    "* Finally, the function calculates the number of frames processed and releases the cv2.VideoCapture object. \n",
    "* It returns a tuple containing the video_frames list, the number of frames processed frame_ct, and the frames per second of the video fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50240cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     frame_ct = len(video_frames)\n",
    "#     cap.release()\n",
    "#     return video_frames, frame_ct, fps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f503451",
   "metadata": {},
   "source": [
    "* **Overall**, \n",
    "* this code reads in a video, detects the face in the first frame, selects the region of interest containing the face in each subsequent frame, and preprocesses the ROI by resizing and normalizing it. \n",
    "* The resulting processed frames are stored in a list and returned as output along with the number of frames and fps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb417663",
   "metadata": {},
   "source": [
    "* **The reason for converting the image to grayscale**\n",
    "* is to simplify the image processing pipeline and reduce the computational complexity of the algorithm. \n",
    "* In general, grayscale images have a single channel (compared to the 3 channels of RGB color images) and only represent the intensity of the image at each pixel, without color information.\n",
    "\n",
    "* In this particular code, the grayscale conversion is also useful for the face detection step, as many face detection algorithms (including the get_frontal_face_detector() method from the dlib library used in this code) are designed to work on grayscale images. \n",
    "* Converting the image to grayscale also reduces the amount of variation in the image that is not relevant to the face detection task, such as changes in color or lighting conditions.\n",
    "\n",
    "* Overall, converting the image to grayscale simplifies the image processing pipeline and makes it easier to detect faces in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7684192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d301d22",
   "metadata": {},
   "source": [
    "* pyramids.py - Contains functions to generate and collapse image/video pyramids (Gaussian/Laplacian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca2f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Build Gaussian image pyramid\n",
    "def build_gaussian_pyramid(img, levels):\n",
    "    float_img = np.ndarray(shape=img.shape, dtype=\"float\")\n",
    "    float_img[:] = img\n",
    "    pyramid = [float_img]\n",
    "\n",
    "    for i in range(levels-1):\n",
    "        float_img = cv2.pyrDown(float_img)\n",
    "        pyramid.append(float_img)\n",
    "\n",
    "    return pyramid\n",
    "\n",
    "\n",
    "# Build Laplacian image pyramid from Gaussian pyramid\n",
    "def build_laplacian_pyramid(img, levels):\n",
    "    gaussian_pyramid = build_gaussian_pyramid(img, levels)\n",
    "    laplacian_pyramid = []\n",
    "\n",
    "    for i in range(levels-1):\n",
    "        upsampled = cv2.pyrUp(gaussian_pyramid[i+1])\n",
    "        (height, width, depth) = upsampled.shape\n",
    "        gaussian_pyramid[i] = cv2.resize(gaussian_pyramid[i], (height, width))\n",
    "        diff = cv2.subtract(gaussian_pyramid[i],upsampled)\n",
    "        laplacian_pyramid.append(diff)\n",
    "\n",
    "    laplacian_pyramid.append(gaussian_pyramid[-1])\n",
    "\n",
    "    return laplacian_pyramid\n",
    "\n",
    "\n",
    "# Build video pyramid by building Laplacian pyramid for each frame\n",
    "def build_video_pyramid(frames):\n",
    "    lap_video = []\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        pyramid = build_laplacian_pyramid(frame, 3)\n",
    "        for j in range(3):\n",
    "            if i == 0:\n",
    "                lap_video.append(np.zeros((len(frames), pyramid[j].shape[0], pyramid[j].shape[1], 3)))\n",
    "            lap_video[j][i] = pyramid[j]\n",
    "\n",
    "    return lap_video\n",
    "\n",
    "\n",
    "# Collapse video pyramid by collapsing each frame's Laplacian pyramid\n",
    "def collapse_laplacian_video_pyramid(video, frame_ct):\n",
    "    collapsed_video = []\n",
    "\n",
    "    for i in range(frame_ct):\n",
    "        prev_frame = video[-1][i]\n",
    "\n",
    "        for level in range(len(video) - 1, 0, -1):\n",
    "            pyr_up_frame = cv2.pyrUp(prev_frame)\n",
    "            (height, width, depth) = pyr_up_frame.shape\n",
    "            prev_level_frame = video[level - 1][i]\n",
    "            prev_level_frame = cv2.resize(prev_level_frame, (height, width))\n",
    "            prev_frame = pyr_up_frame + prev_level_frame\n",
    "\n",
    "        # Normalize pixel values\n",
    "        min_val = min(0.0, prev_frame.min())\n",
    "        prev_frame = prev_frame + min_val\n",
    "        max_val = max(1.0, prev_frame.max())\n",
    "        prev_frame = prev_frame / max_val\n",
    "        prev_frame = prev_frame * 255\n",
    "\n",
    "        prev_frame = cv2.convertScaleAbs(prev_frame)\n",
    "        collapsed_video.append(prev_frame)\n",
    "\n",
    "    return collapsed_video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6386ba1",
   "metadata": {},
   "source": [
    "* The above code defines a set of functions that build Gaussian and Laplacian pyramids for images and videos, and use them to create and collapse pyramids of video frames. \n",
    "* These pyramids are a way of representing an image or video at multiple levels of detail, \n",
    "* where the higher levels contain less detail but more global information, and the lower levels contain more detail but less global information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bdc0fd",
   "metadata": {},
   "source": [
    "* The build_gaussian_pyramid function takes an input image and a number of levels and returns a list of images, \n",
    "* where each image is a smoothed and downsampled version of the previous level. \n",
    "* This can be useful for various image processing tasks, such as image blending or texture synthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5206d3",
   "metadata": {},
   "source": [
    "* The build_laplacian_pyramid function takes an input image and a number of levels and returns a list of images, \n",
    "* where each image represents the difference between the corresponding level of the Gaussian pyramid and the upsampled and resized version of the next level of the Gaussian pyramid. \n",
    "* This can be useful for tasks such as image compression or feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c19782",
   "metadata": {},
   "source": [
    "* The build_video_pyramid function takes a list of frames and builds a Laplacian pyramid for each frame, returning a list of lists of images representing the pyramids for each frame at each level. \n",
    "* This can be useful for tasks such as video compression or motion analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da6e429",
   "metadata": {},
   "source": [
    "* The collapse_laplacian_video_pyramid function takes a Laplacian video pyramid and collapses it by adding up the levels of each frame's pyramid,\n",
    "* starting with the lowest level and working up to the highest. \n",
    "* This can be useful for tasks such as video denoising or super-resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f043e",
   "metadata": {},
   "source": [
    "* **Overall**\n",
    "* these functions provide a powerful toolset for representing and manipulating images and videos at different levels of detail,\n",
    "* allowing for a wide range of image processing and computer vision applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae449fb7",
   "metadata": {},
   "source": [
    "* The above code defines several functions that are used to build and collapse Laplacian image pyramids.\n",
    "\n",
    "* build_gaussian_pyramid(img, levels) function builds a Gaussian pyramid for an input image. A Gaussian pyramid is a multi-resolution image pyramid that contains successive reduced images with decreasing resolution. It is built by repeatedly applying a Gaussian blur filter to the image and down-sampling the image. The function takes an input image and the number of levels in the pyramid as arguments and returns a list of images that make up the pyramid.\n",
    "\n",
    "* build_laplacian_pyramid(img, levels) function builds a Laplacian pyramid for an input image. A Laplacian pyramid is a multi-resolution image pyramid that contains successive high-pass filtered images. It is built by subtracting each level of the Gaussian pyramid from its up-sampled version. The function takes an input image and the number of levels in the pyramid as arguments and returns a list of images that make up the pyramid.\n",
    "\n",
    "* build_video_pyramid(frames) function builds a Laplacian pyramid for each frame in a video. The function takes a list of frames as an argument and returns a list of Laplacian pyramids, one for each frame.\n",
    "\n",
    "* collapse_laplacian_video_pyramid(video, frame_ct) function collapses a Laplacian pyramid for each frame in a video. The function takes a list of Laplacian pyramids and the number of frames in the video as arguments and returns a list of images, one for each frame in the video.\n",
    "\n",
    "* Overall, these functions can be used to build and collapse image pyramids, which can be useful in various computer vision and image processing applications, such as image compression, object detection, and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff490c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6f37052",
   "metadata": {},
   "source": [
    "* eulerian.py - Contains function for a temporal bandpass filter that uses a Fast-Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63550c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.fftpack as fftpack\n",
    "\n",
    "\n",
    "# Temporal bandpass filter with Fast-Fourier Transform\n",
    "def fft_filter(video, freq_min, freq_max, fps):\n",
    "    fft = fftpack.fft(video, axis=0)\n",
    "    frequencies = fftpack.fftfreq(video.shape[0], d=1.0 / fps)\n",
    "    bound_low = (np.abs(frequencies - freq_min)).argmin()\n",
    "    bound_high = (np.abs(frequencies - freq_max)).argmin()\n",
    "    fft[:bound_low] = 0\n",
    "    fft[bound_high:-bound_high] = 0\n",
    "    fft[-bound_low:] = 0\n",
    "    iff = fftpack.ifft(fft, axis=0)\n",
    "    result = np.abs(iff)\n",
    "    result *= 100  # Amplification factor\n",
    "\n",
    "    return result, fft, frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6e2240",
   "metadata": {},
   "source": [
    "* This code defines a function fft_filter that applies a temporal bandpass filter to a video using Fast-Fourier Transform.\n",
    "\n",
    "* The input parameters are:\n",
    "\n",
    "    * video: a 4D numpy array representing a video with dimensions (frames, height, width, channels)\n",
    "    * freq_min: the lower frequency cutoff in Hz\n",
    "    * freq_max: the upper frequency cutoff in Hz\n",
    "    * fps: the frames per second of the video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53081a3",
   "metadata": {},
   "source": [
    "* The function first applies FFT along the first dimension of the video array to obtain the frequency spectrum. \n",
    "* Then, it sets all frequencies outside the range [freq_min, freq_max] to zero in the spectrum. \n",
    "* Finally, it applies inverse FFT along the first dimension to obtain the filtered video. \n",
    "* The filtered video is then multiplied by an amplification factor of 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c569b",
   "metadata": {},
   "source": [
    "* The function returns a tuple containing:\n",
    "\n",
    "    * result: the filtered video as a 4D numpy array with the same dimensions as the input video\n",
    "    * fft: the frequency spectrum of the input video\n",
    "    * frequencies: the frequencies corresponding to each element of the frequency spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7244f4a",
   "metadata": {},
   "source": [
    "* The code above defines a function fft_filter that applies a temporal bandpass filter to a video using Fast Fourier Transform (FFT). The input arguments are:\n",
    "\n",
    "    * video: A numpy array containing the video frames, where the first dimension corresponds to the frame index and the remaining dimensions correspond to the frame shape.\n",
    "    * freq_min: A float specifying the lower frequency cutoff of the filter in Hz.\n",
    "    * freq_max: A float specifying the upper frequency cutoff of the filter in Hz.\n",
    "    * fps: A float specifying the frame rate of the video in frames per second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd073045",
   "metadata": {},
   "source": [
    "* The function first applies the FFT to the video frames along the temporal dimension using the fftpack.fft function from the scipy package. \n",
    "* It then calculates the frequencies of the FFT using the fftpack.fftfreq function. \n",
    "* The lower and upper bounds of the filter are determined based on the specified freq_min and freq_max values using the np.abs and argmin functions. \n",
    "* The frequencies outside of the bounds are set to zero in the FFT. \n",
    "* The inverse FFT is then applied using the fftpack.ifft function, and the absolute value of the result is computed and amplified by a factor of 100. \n",
    "* The resulting filtered video is returned along with the FFT and frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb57d78c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08708e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "000bcbf3",
   "metadata": {},
   "source": [
    "* heartrate.py - Contains function to calculate heart rate from FFT results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a024dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "\n",
    "# Calculate heart rate from FFT peaks\n",
    "def find_heart_rate(fft, freqs, freq_min, freq_max):\n",
    "    fft_maximums = []\n",
    "\n",
    "    for i in range(fft.shape[0]):\n",
    "        if freq_min <= freqs[i] <= freq_max:\n",
    "            fftMap = abs(fft[i])\n",
    "            fft_maximums.append(fftMap.max())\n",
    "        else:\n",
    "            fft_maximums.append(0)\n",
    "\n",
    "    peaks, properties = signal.find_peaks(fft_maximums)\n",
    "    max_peak = -1\n",
    "    max_freq = 0\n",
    "\n",
    "    # Find frequency with max amplitude in peaks\n",
    "    for peak in peaks:\n",
    "        if fft_maximums[peak] > max_freq:\n",
    "            max_freq = fft_maximums[peak]\n",
    "            max_peak = peak\n",
    "\n",
    "    return freqs[max_peak] * 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ef02e1",
   "metadata": {},
   "source": [
    "* This code defines a function called find_heart_rate that takes as input the result of a Fast Fourier Transform (fft), \n",
    "* the frequencies that correspond to each point in the FFT (freqs), \n",
    "* the minimum and maximum frequency values of interest (freq_min and freq_max), and returns an \n",
    "* estimated heart rate in beats per minute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b269356",
   "metadata": {},
   "source": [
    "* First, the function calculates the maximum amplitude of the FFT for each frequency, \n",
    "* but only considers the maximums within the frequency range of interest (freq_min and freq_max), and stores these values in the list fft_maximums."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f807271",
   "metadata": {},
   "source": [
    "* Next, the function uses the find_peaks function from the signal module in SciPy to find the indices of the local maxima in fft_maximums. \n",
    "* These peaks correspond to the most prominent frequencies in the FFT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4096850",
   "metadata": {},
   "source": [
    "* Finally, the function loops over the indices of the peaks to find the peak with the highest amplitude in fft_maximums. \n",
    "* This peak corresponds to the frequency with the highest amplitude in the FFT, and therefore the most likely heart rate. \n",
    "*  The function returns this frequency in beats per minute, which is calculated by multiplying the frequency by 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910af85",
   "metadata": {},
   "source": [
    "* The code defines a function find_heart_rate() that takes in the Fast Fourier Transform (FFT) results, frequencies, the minimum and maximum frequency of the signal of interest, and calculates the heart rate from the FFT peaks.\n",
    "\n",
    "* The function first initializes an empty list fft_maximums to store the maximum FFT values for the frequencies in the range between freq_min and freq_max. It then iterates over the FFT results fft along its first dimension (i.e., time axis), and for each frequency in the range, finds the maximum value in the corresponding FFT map and adds it to fft_maximums. If the frequency is outside the range, 0 is added to fft_maximums.\n",
    "\n",
    "* Next, the function uses signal.find_peaks() from the SciPy signal processing module to find the peaks in fft_maximums. The function then finds the maximum peak in fft_maximums and returns the corresponding frequency, multiplied by 60 to convert it to beats per minute, as the heart rate.\n",
    "\n",
    "* In summary, the find_heart_rate() function finds the heart rate from the FFT peaks of a signal in the frequency range of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a7d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a7dfbbf",
   "metadata": {},
   "source": [
    "* The main.py file contains the main program that utilizes all of the other modules defined in the other code files to read in the input video, run Eulerian magnification on it, and to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "289d9a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading + preprocessing video...\n",
      "Building Laplacian video pyramid...\n",
      "Running FFT and Eulerian magnification...\n",
      "Calculating heart rate...\n",
      "Rebuilding final video...\n",
      "Heart rate:  62.30031948881789 bpm\n",
      "Displaying final video...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# import pyramids\n",
    "# import heartrate\n",
    "# import preprocessing\n",
    "# import eulerian\n",
    "\n",
    "# Frequency range for Fast-Fourier Transform\n",
    "freq_min = 1\n",
    "freq_max = 1.8\n",
    "\n",
    "# Preprocessing phase\n",
    "print(\"Reading + preprocessing video...\")\n",
    "# video_frames, frame_ct, fps = preprocessing.read_video(\"videos/rohin_active.mov\")\n",
    "video_frames, frame_ct, fps = read_video(\"videos/rohin_active.mov\")\n",
    "\n",
    "# Build Laplacian video pyramid\n",
    "print(\"Building Laplacian video pyramid...\")\n",
    "# lap_video = pyramids.build_video_pyramid(video_frames)\n",
    "lap_video = build_video_pyramid(video_frames)\n",
    "\n",
    "amplified_video_pyramid = []\n",
    "\n",
    "for i, video in enumerate(lap_video):\n",
    "    if i == 0 or i == len(lap_video)-1:\n",
    "        continue\n",
    "\n",
    "    # Eulerian magnification with temporal FFT filtering\n",
    "    print(\"Running FFT and Eulerian magnification...\")\n",
    "#     result, fft, frequencies = eulerian.fft_filter(video, freq_min, freq_max, fps)\n",
    "    result, fft, frequencies = fft_filter(video, freq_min, freq_max, fps)\n",
    "    lap_video[i] += result\n",
    "\n",
    "    # Calculate heart rate\n",
    "    print(\"Calculating heart rate...\")\n",
    "#     heart_rate = heartrate.find_heart_rate(fft, frequencies, freq_min, freq_max)\n",
    "    heart_rate = find_heart_rate(fft, frequencies, freq_min, freq_max)\n",
    "\n",
    "# Collapse laplacian pyramid to generate final video\n",
    "print(\"Rebuilding final video...\")\n",
    "# amplified_frames = pyramids.collapse_laplacian_video_pyramid(lap_video, frame_ct)\n",
    "amplified_frames = collapse_laplacian_video_pyramid(lap_video, frame_ct)\n",
    "\n",
    "# Output heart rate and final video\n",
    "print(\"Heart rate: \", heart_rate, \"bpm\")\n",
    "print(\"Displaying final video...\")\n",
    "\n",
    "for frame in amplified_frames:\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    cv2.waitKey(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f762e6",
   "metadata": {},
   "source": [
    "* This is a Python code for reading and pre-processing video frames using OpenCV and dlib. The code performs the following tasks:\n",
    "    * Load the face detector from dlib.\n",
    "    * Read in the video from the given path and calculate the FPS of the video.\n",
    "    * Pre-process each frame of the video by converting it to grayscale and detecting the face in the first frame.\n",
    "    * Select the region of interest (ROI) around the detected face and resize it to (500, 500).\n",
    "    * Normalize the ROI to a range of [0, 1] and append it to a list of pre-processed frames.\n",
    "    \n",
    "* The function read_video() takes a single argument, path, which is the path to the video file. The function returns a tuple consisting of:\n",
    "    * a list of pre-processed frames\n",
    "    * the total number of frames in the video\n",
    "    * the FPS of the video.\n",
    "    \n",
    "* Note that the code assumes that there is only one face in the video, and it only detects the face in the first frame. \n",
    "* If there are multiple faces or the face moves out of the frame, the code will not be able to detect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837f08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
